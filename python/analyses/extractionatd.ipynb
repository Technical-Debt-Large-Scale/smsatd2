{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Technical-Debt-Large-Scale/qualification/blob/main/python/analysis/extractionatd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Technical-Debt-Large-Scale/qualification.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "file_xls_to_scan = 'qualification/xls/mergepatdsp.xls'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leitura dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_my_xls_data = pd.read_excel(file_xls_to_scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_my_xls_data\n",
    "df_bib_sort_by_year = df_data.copy()\n",
    "df_bib_sort_by_year['year'] = pd.to_numeric(df_bib_sort_by_year['year'])\n",
    "df_bib_sort_by_year = df_bib_sort_by_year.sort_values('year')\n",
    "\n",
    "df_data = df_bib_sort_by_year.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisa os conteudos dos Abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analise via NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove os abstracts vazios\n",
    "df_data = df_data[['key', 'year', 'list_authors', 'title', 'abstract', 'link']]\n",
    "df_data = df_data.dropna()\n",
    "\n",
    "df_all_papers = df_data.copy()\n",
    "df_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala a suite NLTK https://www.nltk.org/\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatena todos os abstracts em uma unica sentenca\n",
    "list_abstract = df_data.abstract.to_list()\n",
    "all_abstract_in_one = ' '.join(list_abstract)\n",
    "\n",
    "# Tokenization\n",
    "tokens_abstract = nltk.word_tokenize(all_abstract_in_one)\n",
    "tokens_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte todos os tokens de todos os abstracts em minusculo\n",
    "[item.lower() for item in tokens_abstract]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recupera regras de frases em ingles\n",
    "tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
    "tokenizer.tokenize('Hello.  This is a test.  It works!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guarda em uma lista todas as frases que contem Architectural Technical Debt\n",
    "list_atd = []\n",
    "list_atd_aux = []\n",
    "for abstract in list_abstract:\n",
    "  list_temp = tokenizer.tokenize(abstract)\n",
    "  for item in list_temp:\n",
    "    item_to_lower = item.lower()\n",
    "    atd_lower = 'Architectural Technical Debt'.lower()\n",
    "    if atd_lower in item_to_lower:\n",
    "      list_atd.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz o merge de todos as sentencas que contem ATD\n",
    "all_atd_abstract_in_one = ' '.join(list_atd)\n",
    "\n",
    "# Tokenization\n",
    "tokens_atd_abstract = nltk.word_tokenize(all_atd_abstract_in_one)\n",
    "\n",
    "# Lista de todos os tokens de todos os atd abstracts em minusculo\n",
    "list_of_tokens_atd_abstract = [item.lower() for item in tokens_atd_abstract]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz o merge de todos as sentencas que contem ATD\n",
    "all_atd_abstract_in_one = ' '.join(list_atd)\n",
    "\n",
    "# Tokenization\n",
    "tokens_atd_abstract = nltk.word_tokenize(all_atd_abstract_in_one)\n",
    "\n",
    "# Lista de todos os tokens de todos os atd abstracts em minusculo\n",
    "list_of_tokens_atd_abstract = [item.lower() for item in tokens_atd_abstract]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz a contagem das palavras e guarda em um dicionario\n",
    "from collections import Counter\n",
    "\n",
    "counts_atd_words = Counter(list_of_tokens_atd_abstract)\n",
    "counts_atd_words\n",
    "\n",
    "dict_counts_atd_words = dict(counts_atd_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordena o dicionario de palavras por valor crescente\n",
    "# Mostra as palavras que mais se repetem\n",
    "sorted(dict_counts_atd_words.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove os stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_atd_abstract_in_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "new_words = tokenizer.tokenize(all_atd_abstract_in_one)\n",
    "\n",
    "#word_tokens = word_tokenize(all_atd_abstract_in_one)\n",
    "word_tokens = new_words \n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    " \n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de todos os tokens de todos os atd abstracts em minusculo sem o stop words\n",
    "list_of_tokens_atd_abstract_no_ponctuation = [item.lower() for item in filtered_sentence]\n",
    "\n",
    "# Faz a contagem das palavras e guarda em um dicionario\n",
    "from collections import Counter\n",
    "\n",
    "counts_atd_words_no_ponctuation = Counter(list_of_tokens_atd_abstract_no_ponctuation)\n",
    "\n",
    "dict_counts_atd_words_no_ponctuation = dict(counts_atd_words_no_ponctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordena o dicionario de palavras, sem os stop words, por valor crescente \n",
    "sorted(dict_counts_atd_words_no_ponctuation.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word *cloud*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/amueller/word_cloud.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd word_cloud && pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a Word of Cloud about each file according frequence\n",
    "def generateWordCloud(name, counterWithFrequency, my_path='qualification/wordcloud'):\n",
    "  try:\n",
    "    wordcloud = WordCloud(width = 1200, height = 1000, random_state=1, background_color='black', colormap='Set2', collocations=False)\n",
    "    wordcloud.generate_from_frequencies(frequencies=counterWithFrequency)\n",
    "    # Display the generated image:\n",
    "    fileName = my_path + '/' + name + \".png\"\n",
    "    wordcloud.to_file(fileName)\n",
    "    print(f'Arquivo {fileName} gerado com sucesso!')\n",
    "  except Exception as ex:\n",
    "    print(f'Erro ao gerar o arquivo {name}: {str(ex)}')\n",
    "\n",
    "# Generate a Word of Cloud about text\n",
    "def generateWordCloud2(name, my_text, my_path='qualification/wordcloud'):\n",
    "  try:\n",
    "    wordcloud = WordCloud(width = 1200, height = 1000, random_state=1, background_color='black', colormap='Set2', collocations=False)\n",
    "    wordcloud.generate(my_text)\n",
    "    # Display the generated image:\n",
    "    fileName = my_path + '/' + name + \".png\"\n",
    "    wordcloud.to_file(fileName)\n",
    "    print(f'Arquivo {fileName} gerado com sucesso!')\n",
    "  except Exception as ex:\n",
    "    print(f'Erro ao gerar o arquivo {name}: {str(ex)}')\n",
    "\n",
    "# Generate a Word of Cloud about text\n",
    "def generateWordCloud3(name, my_text, my_path='qualification/wordcloud'):\n",
    "  try:\n",
    "    wordcloud = WordCloud(width=1200, height=1000).generate(my_text)\n",
    "    wordcloud.generate(my_text)\n",
    "    # Display the generated image:\n",
    "    fileName = my_path + '/' + name + \".png\"\n",
    "    wordcloud.to_file(fileName)\n",
    "    print(f'Arquivo {fileName} gerado com sucesso!')\n",
    "  except Exception as ex:\n",
    "    print(f'Erro ao gerar o arquivo {name}: {str(ex)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateWordCloud('word_cloud_atd_no_ponctuation', dict_counts_atd_words_no_ponctuation)\n",
    "generateWordCloud2('word_cloud_atd_all_in_one', all_atd_abstract_in_one)\n",
    "generateWordCloud3('word_cloud_atd_all_in_one2', all_atd_abstract_in_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisa apenas os papers que tenham ATD no Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_all_papers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_with_atd = df_all_papers.abstract.str.contains('Architectural Technical Debt', case=False)\n",
    "df_atd_papers = df_all_papers[abstract_with_atd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atd_papers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atd_papers.groupby(['year']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atd_papers[['key', 'year', 'title', 'list_authors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma lista de elementos contendo a key e as setencas do abstract que contem ATD\n",
    "list_key_abstract_atd = []\n",
    "for each in list_atd:\n",
    "  for index, row in df_atd_papers.iterrows():\n",
    "    if each in row['abstract']:\n",
    "      elemento = (row['key'], each)\n",
    "      list_key_abstract_atd.append(elemento)\n",
    "    \n",
    "# Cria um dicionario onde a chave é o author principal e o value é a lista de conceitos de ATD\n",
    "dict_key_abstract_atd = {}\n",
    "list_of_key = []\n",
    "\n",
    "for each_elemento in list_key_abstract_atd:\n",
    "  list_of_key.append(each_elemento[0])\n",
    "set_of_key = set(list_of_key)\n",
    "list_of_key = list(set_of_key)\n",
    "\n",
    "list_sentence_aux = []\n",
    "for item in list_of_key: \n",
    "  for each_elemento in list_key_abstract_atd:\n",
    "    if item==each_elemento[0]:\n",
    "      list_sentence_aux.append(each_elemento[1])\n",
    "  dict_key_abstract_atd[item] = list_sentence_aux\n",
    "  list_sentence_aux = []\n",
    "\n",
    "# Cria um dicionario contendo key, authors, year, title e conceito de atd\n",
    "dict_key_authors_year_title_atd = {}\n",
    "list_atd_key = []\n",
    "list_atd_authors = []\n",
    "list_atd_year = []\n",
    "list_atd_title = []\n",
    "list_atd_concept = []\n",
    "list_atd_link = []\n",
    "for key, value in dict_key_abstract_atd.items():\n",
    "  for index, row in df_atd_papers.iterrows():\n",
    "    if key==row['key']:\n",
    "      list_atd_key.append(key)\n",
    "      list_atd_authors.append(row['list_authors'])\n",
    "      list_atd_year.append(row['year'])\n",
    "      list_atd_title.append(row['title'])\n",
    "      list_atd_concept.append(value)\n",
    "      list_atd_link.append(row['link'])\n",
    "\n",
    "dict_key_authors_year_title_atd = {'key':list_atd_key, 'year':list_atd_year, 'title':list_atd_title, 'authors':list_atd_authors, 'atd':list_atd_concept, 'link':list_atd_link}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_key_authors_year_title_atd = pd.DataFrame(dict_key_authors_year_title_atd)\n",
    "df_key_authors_year_title_atd.sort_values(by='year')[['year', 'title', 'authors', 'atd']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_html_file(arquivo_html, df_data):\n",
    "    print('Criando o arquivo {}'.format(arquivo_html))\n",
    "    try: \n",
    "        with open(arquivo_html, 'w', encoding='utf-8') as file_html:\n",
    "            file_html.write('<html>')\n",
    "            file_html.write('<head>')\n",
    "            file_html.write('<meta charset=\"utf-8\">')\n",
    "            file_html.write('<title>Lista de Papers mais importantes sobre ATD</title>')\n",
    "            file_html.write('</head>')\n",
    "            file_html.write('<body>')\n",
    "            for index, row in df_data.iterrows():\n",
    "                title = '<h2>Título ' + str(index+1) + ': ' + row['title'] + '</h2>'\n",
    "                file_html.write(title)\n",
    "                authors = '<h3>Autores: ' + row['authors'] + '</h3>'\n",
    "                file_html.write(authors)\n",
    "                year = str(row['year'])\n",
    "                year = '<h4>Ano: ' + year +  '</h4>'\n",
    "                file_html.write(year)\n",
    "                if row['atd'] is not None:\n",
    "                  abstract = '<p>' + str(row['atd']) + '</p>'\n",
    "                else:\n",
    "                    abstract = ''\n",
    "                file_html.write(abstract)\n",
    "                file_html.write('<br>')\n",
    "                if row['link'] is not None:\n",
    "                  link = '<a href=\"' + str(row['link']) + '\">link</a>'\n",
    "                else:\n",
    "                    link = ''\n",
    "                file_html.write(link)\n",
    "                file_html.write('<br>')\n",
    "            file_html.write('</body>')\n",
    "            file_html.write('</html>')\n",
    "            print('Arquivo criado com sucesso!')\n",
    "    except Exception as e: \n",
    "        print('Erro {} ao criar o arquivo {}'.format(str(e), arquivo_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_key_authors_year_title_atd = df_key_authors_year_title_atd.sort_values(by='year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_html_file(arquivo_html='qualification/html/atd_concepts.html', df_data=df_key_authors_year_title_atd)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "05ece30799c2dcdac4c13b3af20453da19de8df0d9a1de52cff7e0b6e1e82bdd"
  },
  "kernelspec": {
   "display_name": "Python 3.6.3 64-bit ('3.6')",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
